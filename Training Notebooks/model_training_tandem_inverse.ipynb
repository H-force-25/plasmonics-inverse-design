{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tapxaurdc5rp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import and modify dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FMk3vku6c9te",
        "outputId": "02d68110-64cf-48ed-e5be-2def80b0f81b"
      },
      "outputs": [],
      "source": [
        "ds = pd.read_csv('../Dataset/data_abs.csv', header=None)\n",
        "ds = ds[ds[4] == 1] # Select data for water media\n",
        "ds = ds[ds[1] != 0] # Select only core-shell particles\n",
        "ds = ds.sample(frac=1).reset_index(drop=True) # Randomize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzlCyWS-fsUo"
      },
      "outputs": [],
      "source": [
        "num_mats = 11 # Total number of materials used (including \"empty\")\n",
        "spectra = np.array(ds.iloc[:, 5:])/255 # Normalized spectra\n",
        "\n",
        "m1 = np.array(ds[[0]]) # Core material ids\n",
        "m2 = np.array(ds[[1]]) # Shell material ids\n",
        "t1 = np.array(ds[[2]])/100 # Normalized core thickness\n",
        "t2 = np.array(ds[[3]])/100 # Normalized shell thickness\n",
        "\n",
        "geo = np.hstack((m1, m2, t1, t2)) # Geometry parameters\n",
        "\n",
        "test_split = int(0.95*ds.shape[0]) # Number of samples separated for testing after training\n",
        "\n",
        "# Training spectra\n",
        "y_train = spectra[:test_split]\n",
        "y_train = tf.expand_dims(y_train, axis=-1)\n",
        "\n",
        "# Training geometry parameters\n",
        "x_train_m1 = keras.utils.to_categorical(m1[:test_split])\n",
        "x_train_m1 = tf.expand_dims(x_train_m1, axis=1)\n",
        "x_train_m2 = keras.utils.to_categorical(m2[:test_split])\n",
        "x_train_m2 = tf.expand_dims(x_train_m2, axis=1)\n",
        "x_train_t1 = t1[:test_split]\n",
        "x_train_t1 = tf.expand_dims(x_train_t1, axis=-1)\n",
        "x_train_t2 = t2[:test_split]\n",
        "x_train_t2 = tf.expand_dims(x_train_t2, axis=-1)\n",
        "x_train = geo[:test_split]\n",
        "x_train = tf.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Testing spectra\n",
        "y_test = spectra[test_split:]\n",
        "y_test = tf.expand_dims(y_test, axis=-1)\n",
        "\n",
        "# Testing geometry parameters\n",
        "x_test_m1 = keras.utils.to_categorical(m1[test_split:])\n",
        "x_test_m1 = tf.expand_dims(x_test_m1, axis=1)\n",
        "x_test_m2 = keras.utils.to_categorical(m2[test_split:])\n",
        "x_test_m2 = tf.expand_dims(x_test_m2, axis=1)\n",
        "x_test_t1 = t1[test_split:]\n",
        "x_test_t1 = tf.expand_dims(x_test_t1, axis=-1)\n",
        "x_test_t2 = t2[test_split:]\n",
        "x_test_t2 = tf.expand_dims(x_test_t2, axis=-1)\n",
        "x_test = geo[test_split:]\n",
        "x_test = tf.expand_dims(x_test, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-0Ci-zkru_"
      },
      "source": [
        "# Load pre-trained forward model and freeze weights\n",
        "\n",
        "The forward model is pre-trained, we do not want it to be modified (further trained) in the inverse net training.\n",
        "\n",
        "We therefore need to set the forward model as not trainable. Note: If a full model is set to trainable = False, the model needs to be compiled, for it to have an effect. Setting the model to trainable = False after compilation, this does not have any effect. To avoid recompilation, every single layer can be set to trainable=False in a loop through the model layers, which works also on compiled models without re-compilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bzPc6RiCk_l"
      },
      "outputs": [],
      "source": [
        "model_path = '../Models/model_tandem_forward.h5'\n",
        "\n",
        "# load pretrained forward model\n",
        "fwd_model = keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "fwd_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GHJrdFlC1nE"
      },
      "source": [
        "# Inverse resnet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2HVLecvQpfq"
      },
      "outputs": [],
      "source": [
        "def residual_block(x_in, N_filter, kernel_size=3, strides=1,\n",
        "                   conv_layer=keras.layers.Conv1D, alpha=0.3,\n",
        "                   with_BN=False):\n",
        "    \n",
        "    # Residual connection\n",
        "    if x_in.shape[-1] != N_filter or strides != 1:\n",
        "        # If input != output dimension, add BN/ReLU/conv. into shortcut\n",
        "        conv_shortcut = conv_layer(\n",
        "            filters=N_filter, kernel_size=1, strides=strides, padding='same')(x_in)\n",
        "    else:\n",
        "        # Else use bare input as shortcut\n",
        "        conv_shortcut = x_in\n",
        "\n",
        "    # Convolutional path\n",
        "    x = x_in\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=1, strides=1,\n",
        "                   padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=kernel_size,\n",
        "                   strides=strides, padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=1, strides=1,\n",
        "                   padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Add residual and main and apply a further activation\n",
        "    x = keras.layers.Add()([x, conv_shortcut])\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrbSrNT7QmR7"
      },
      "outputs": [],
      "source": [
        "def resblock_sequence_down(x_in, N_filter, N_blocks):\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    # Use strides=2 for downsampling (more flexible, since trainable).\n",
        "    x = residual_block(x, N_filter, kernel_size=3, strides=2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resblock_sequence_up(x_in, N_filter, N_blocks):\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    # Use upsampling (more robust than transpose convolutions with stride 2)\n",
        "    x = keras.layers.UpSampling1D()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resblock_sequence_id(x_in, N_filter, N_blocks):\n",
        "    # Identity block sequence: input shape = output shape\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPR2GTWoC3PH",
        "outputId": "b9092141-81e9-4d9e-9720-7ab5e0010a09"
      },
      "outputs": [],
      "source": [
        "N_blocks = 3 # Number of resblocks between upsamplings\n",
        "\n",
        "# Spectrum in\n",
        "target_spec_input = keras.layers.Input(shape=y_train.shape[1:])\n",
        "x = target_spec_input\n",
        "\n",
        "for i in range(N_blocks):\n",
        "    x = residual_block(x, 32, kernel_size=3, strides=1)\n",
        "# Use strides=3 for downsampling. 200 --> 67\n",
        "x = residual_block(x, 32, kernel_size=3, strides=3)\n",
        "\n",
        "for i in range(N_blocks):\n",
        "    x = residual_block(x, 64, kernel_size=3, strides=1)\n",
        "x = residual_block(x, 64, kernel_size=3, strides=2) # 67 --> 34\n",
        "\n",
        "for i in range(N_blocks):\n",
        "    x = residual_block(x, 128, kernel_size=3, strides=1)\n",
        "x = residual_block(x, 128, kernel_size=3, strides=2) # 34 --> 12\n",
        "\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x_m1 = keras.layers.Dense(256, activation='relu')(x)\n",
        "x_m1 = keras.layers.Dense(256, activation='relu')(x_m1)\n",
        "out_design_m1 = keras.layers.Dense(num_mats, activation='softmax', name='design_out_m1')(x_m1)\n",
        "\n",
        "x_m2 = keras.layers.Dense(256, activation='relu')(x)\n",
        "x_m2 = keras.layers.Dense(256, activation='relu')(x_m2)\n",
        "out_design_m2 = keras.layers.Dense(num_mats, activation='softmax', name='design_out_m2')(x_m2)\n",
        "\n",
        "x_t1 = keras.layers.Dense(256, activation='relu')(x)\n",
        "x_t1 = keras.layers.Dense(256, activation='relu')(x_t1)\n",
        "out_design_t1 = keras.layers.Dense(1, activation='linear', name='design_out_t1')(x_t1)\n",
        "\n",
        "x_t2 = keras.layers.Dense(256, activation='relu')(x)\n",
        "x_t2 = keras.layers.Dense(256, activation='relu')(x_t2)\n",
        "out_design_t2 = keras.layers.Dense(1, activation='linear', name='design_out_t2')(x_t2)\n",
        "\n",
        "inverse_model = keras.Model(\n",
        "    inputs = target_spec_input,\n",
        "    outputs = [out_design_m1, out_design_m2, out_design_t1, out_design_t2], name='resnet_inverse_model')\n",
        "inverse_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbFUwxWQWpaA"
      },
      "source": [
        "# Define the full tandem model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXyM31oNWrA1",
        "outputId": "8df88f4d-2fc1-46da-e707-89fffc75b1d7"
      },
      "outputs": [],
      "source": [
        "# Get the inputs\n",
        "tandem_input = inverse_model.inputs\n",
        "inverse_output = inverse_model(tandem_input)\n",
        "\n",
        "tandem_output = fwd_model([tf.expand_dims(inverse_output[0], axis=1),\n",
        "                           tf.expand_dims(inverse_output[1], axis=1),\n",
        "                           tf.expand_dims(inverse_output[2], axis=1),\n",
        "                           tf.expand_dims(inverse_output[3], axis=1)])\n",
        "\n",
        "tandem = keras.models.Model(inputs=tandem_input, outputs=tandem_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Urj6TwZDIhs"
      },
      "source": [
        "# Tandem training\n",
        "\n",
        "Now we compile the model with an optimizer (AdamW), add callbacks and then run the training.\n",
        "\n",
        "Note on training data: The full tandem model takes as input the design target spectrum and returns the predicted spectrum of the suggested geometry. Therefore we do not use the geometries for training anymore, but we use the spectra for both, in- and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms2NkmZ0DGGU",
        "outputId": "3808a5ae-9433-4b7f-8c50-e9a4ca9e9724"
      },
      "outputs": [],
      "source": [
        "# Compile with optimizer and cost function\n",
        "tandem.compile(optimizer=keras.optimizers.AdamW(learning_rate=0.001), loss='mse', metrics='mse')\n",
        "\n",
        "# Automatic learning rate reduction on loss plateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=6, verbose=1)\n",
        "# Callback for early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
        "# callback for checkpoint saving\n",
        "checkpoint_save = ModelCheckpoint(filepath='checkpoints/tandem/',\n",
        "                                  monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
        "\n",
        "callbacks = [reduce_lr, early_stop, checkpoint_save]\n",
        "\n",
        "hist = None  # Global history after BS incrase\n",
        "for i in range(3):  # 3x16 epochs, doubling batchsize\n",
        "    _h = tandem.fit(x=y_train, y=y_train,\n",
        "                    validation_split=.2,\n",
        "                    batch_size=32 * 2**i, epochs=16,\n",
        "                    callbacks = callbacks)\n",
        "    if hist is None:\n",
        "        hist = _h\n",
        "    else:\n",
        "        # Update history\n",
        "        for k in hist.history:\n",
        "            hist.history[k] = np.concatenate([hist.history[k], _h.history[k]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4i5uWhCGpes"
      },
      "source": [
        "# Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "PGWM_kL0vLTw",
        "outputId": "b97dbc9e-b919-4ae3-895f-56b711dbc703"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(hist.history['loss'], label='loss')\n",
        "plt.plot(hist.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlyQHx33DVBi",
        "outputId": "6cf00502-13f1-4f7c-802c-03989da0278a"
      },
      "outputs": [],
      "source": [
        "tandem.save('../Models/model_tandem.h5')\n",
        "inverse_model.save('../Models/model_tandem_inverse.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
