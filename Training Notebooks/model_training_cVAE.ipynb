{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tapxaurdc5rp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import and modify dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FMk3vku6c9te",
        "outputId": "413a0fad-1ead-4eab-d680-38d4947f4796"
      },
      "outputs": [],
      "source": [
        "ds = pd.read_csv('../Dataset/data_abs.csv', header=None)\n",
        "ds = ds[ds[4] == 1] # Select data for water media\n",
        "ds = ds[ds[1] != 0] # Select only core-shell particles\n",
        "ds = ds.sample(frac=1).reset_index(drop=True) # Randomize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzlCyWS-fsUo"
      },
      "outputs": [],
      "source": [
        "num_mats = 11 # Total number of materials used (including \"empty\")\n",
        "spectra = np.array(ds.iloc[:, 5:])/255 # Normalized spectra\n",
        "\n",
        "m1 = np.array(ds[[0]]) # Core material ids\n",
        "m2 = np.array(ds[[1]]) # Shell material ids\n",
        "t1 = np.array(ds[[2]])/100 # Normalized core thickness\n",
        "t2 = np.array(ds[[3]])/100 # Normalized shell thickness\n",
        "\n",
        "geo = np.hstack((m1, m2, t1, t2)) # Geometry parameters\n",
        "\n",
        "test_split = int(0.95*ds.shape[0]) # Number of samples separated for testing after training\n",
        "\n",
        "# Training output\n",
        "y_train = spectra[:test_split]\n",
        "y_train = tf.expand_dims(y_train, axis=-1)\n",
        "\n",
        "# Training features\n",
        "x_train_m1 = keras.utils.to_categorical(m1[:test_split])\n",
        "x_train_m1 = tf.expand_dims(x_train_m1, axis=1)\n",
        "x_train_m2 = keras.utils.to_categorical(m2[:test_split])\n",
        "x_train_m2 = tf.expand_dims(x_train_m2, axis=1)\n",
        "x_train_t1 = t1[:test_split]\n",
        "x_train_t1 = tf.expand_dims(x_train_t1, axis=-1)\n",
        "x_train_t2 = t2[:test_split]\n",
        "x_train_t2 = tf.expand_dims(x_train_t2, axis=-1)\n",
        "x_train = geo[:test_split]\n",
        "x_train = tf.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Testing output\n",
        "y_test = spectra[test_split:]\n",
        "y_test = tf.expand_dims(y_test, axis=-1)\n",
        "\n",
        "# Testing features\n",
        "x_test_m1 = keras.utils.to_categorical(m1[test_split:])\n",
        "x_test_m1 = tf.expand_dims(x_test_m1, axis=1)\n",
        "x_test_m2 = keras.utils.to_categorical(m2[test_split:])\n",
        "x_test_m2 = tf.expand_dims(x_test_m2, axis=1)\n",
        "x_test_t1 = t1[test_split:]\n",
        "x_test_t1 = tf.expand_dims(x_test_t1, axis=-1)\n",
        "x_test_t2 = t2[test_split:]\n",
        "x_test_t2 = tf.expand_dims(x_test_t2, axis=-1)\n",
        "x_test = geo[test_split:]\n",
        "x_test = tf.expand_dims(x_test, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-8ewFNLkmRG"
      },
      "source": [
        "#  cVAE resnet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9JG-dekkIE"
      },
      "outputs": [],
      "source": [
        "def residual_block(x_in, N_filter, kernel_size=3, strides=1,\n",
        "                   conv_layer=keras.layers.Conv1D, alpha=0.3,\n",
        "                   with_BN=False):\n",
        "\n",
        "    # Residual connection\n",
        "    if x_in.shape[-1] != N_filter or strides != 1:\n",
        "        # If input != output dimension, add BN/ReLU/conv. into shortcut\n",
        "        conv_shortcut = conv_layer(\n",
        "            filters=N_filter, kernel_size=1, strides=strides, padding='same')(x_in)\n",
        "    else:\n",
        "        # Else use bare input as shortcut\n",
        "        conv_shortcut = x_in\n",
        "\n",
        "    # Convolutional path\n",
        "    x = x_in\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=1, strides=1,\n",
        "                   padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=kernel_size,\n",
        "                   strides=strides, padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    x = conv_layer(filters=N_filter, kernel_size=1, strides=1,\n",
        "                   padding='same', use_bias=not with_BN)(x)\n",
        "    if with_BN:\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Add residual and main and apply a further activation\n",
        "    x = keras.layers.Add()([x, conv_shortcut])\n",
        "    x = keras.layers.LeakyReLU(alpha)(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNz5STrXkv_5"
      },
      "outputs": [],
      "source": [
        "def resblock_sequence_down(x_in, N_filter, N_blocks):\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    # Use strides=2 for downsampling (more flexible, since trainable).\n",
        "    x = residual_block(x, N_filter, kernel_size=3, strides=2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resblock_sequence_up(x_in, N_filter, N_blocks):\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    # Use upsampling (more robust than transpose convolutions with stride 2)\n",
        "    x = keras.layers.UpSampling1D()(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resblock_sequence_id(x_in, N_filter, N_blocks):\n",
        "    # Identity block sequence: input shape = output shape\n",
        "    x = x_in\n",
        "    for i in range(N_blocks):\n",
        "        x = residual_block(x, N_filter, kernel_size=3, strides=1, with_BN=True)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-0Ci-zkru_"
      },
      "source": [
        "#  RNG sampling layer for regularization\n",
        "\n",
        "The latent space of a VAE is regularized via random sampling from a normal distribution during training and ragularization of these distributions using a KL loss. The KL loss will be introduced below.\n",
        "Here we define a sampling layer that generates randomly perturbed latent vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM-gxwc9krZ4"
      },
      "outputs": [],
      "source": [
        "class SamplingRNG(keras.layers.Layer):\n",
        "    # Layer to sample normally distributed random numbers z with mean `z_mean` and logarithmic variance `z_log_var`\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_var_log = inputs\n",
        "        epsilon = keras.backend.random_normal(shape=tf.shape(z_mean)[1:])\n",
        "        z = z_mean + keras.backend.exp(0.5 * z_var_log) * epsilon\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg1x-bWgk3bh"
      },
      "source": [
        "# Define the encoder\n",
        "\n",
        "The cVAE consists of first an encoder. This compresses the condition (target spectrum) and the geometry (design) into a latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmRZQ4TEkkUm",
        "outputId": "04ceeea1-b90f-4405-c76d-ab428b8cc92f"
      },
      "outputs": [],
      "source": [
        "latent_dim = 2 # for interpretability\n",
        "N_blocks = 3 # Number of resblocks between upsamplings\n",
        "\n",
        "# Input layers\n",
        "condition_target_spec_input = keras.layers.Input(\n",
        "    shape=y_train.shape[1:], name='condition_in')\n",
        "design_in_m1 = keras.Input(shape=x_train_m1.shape[1:], name='design_in_m1')\n",
        "design_in_m2 = keras.Input(shape=x_train_m2.shape[1:], name='design_in_m2')\n",
        "design_in_t1 = keras.Input(shape=x_train_t1.shape[1:], name='design_in_t1')\n",
        "design_in_t2 = keras.Input(shape=x_train_t2.shape[1:], name='design_in_t2')\n",
        "\n",
        "# Spectrum path\n",
        "x = condition_target_spec_input\n",
        "x = resblock_sequence_down(x, N_filter=16, N_blocks=N_blocks) # 64 --> 32\n",
        "x = resblock_sequence_down(x, N_filter=32, N_blocks=N_blocks) # 32 --> 16\n",
        "x = resblock_sequence_down(x, N_filter=64, N_blocks=N_blocks) # 16 --> 8\n",
        "x = resblock_sequence_down(x, N_filter=128, N_blocks=N_blocks) # 8 --> 4\n",
        "\n",
        "x_spec = keras.layers.Flatten()(x)\n",
        "\n",
        "# Geometry path\n",
        "x_m1 = design_in_m1\n",
        "x_m2 = design_in_m2\n",
        "x_t1 = design_in_t1\n",
        "x_t2 = design_in_t2\n",
        "\n",
        "x_1 = keras.layers.Concatenate(axis=-1)([x_m1, x_t1])\n",
        "x_2 = keras.layers.Concatenate(axis=-1)([x_m2, x_t2])\n",
        "x = keras.layers.Concatenate(axis=1)([x_1, x_2])\n",
        "\n",
        "x = resblock_sequence_id(x, N_filter=64, N_blocks=N_blocks) \n",
        "\n",
        "x_design = keras.layers.Flatten()(x)\n",
        "\n",
        "# Merge both paths\n",
        "x = keras.layers.Concatenate()([x_spec, x_design])\n",
        "x = keras.layers.Dense(256)(x)\n",
        "x = keras.layers.LeakyReLU(alpha=1e-1)(x)\n",
        "\n",
        "# Output: latent sampling\n",
        "z_mean = keras.layers.Dense(latent_dim)(x)\n",
        "z_var_log = keras.layers.Dense(latent_dim)(x)\n",
        "z = SamplingRNG()([z_mean, z_var_log])\n",
        "\n",
        "encoder = keras.Model(\n",
        "    [design_in_m1, design_in_m2, design_in_t1, design_in_t2, condition_target_spec_input],\n",
        "    [z_mean, z_var_log, z],\n",
        "    name=\"encoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgY_5_FXk7LI"
      },
      "source": [
        "# Define the decoder\n",
        "\n",
        "The second stage of the cVAE, and the actual inverse model after training, is the decoder.  It takes as input the condition (target spectrum) and a latent vector z. During training multiple design solutions for a given target spectrum will be mapped on different latent vectors z so that the latent space can be used to distinguish different solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_FWVuNukkau",
        "outputId": "dbe234de-5b73-4131-dabb-bad451204fdc"
      },
      "outputs": [],
      "source": [
        "# Latent vector input\n",
        "latent_in = keras.Input(shape=(latent_dim,), name='latent_in')\n",
        "\n",
        "# Spectrum path\n",
        "x = condition_target_spec_input\n",
        "x = resblock_sequence_down(x, N_filter=16, N_blocks=N_blocks) # 64 --> 32\n",
        "x = resblock_sequence_down(x, N_filter=32, N_blocks=N_blocks) # 32 --> 16\n",
        "x = resblock_sequence_down(x, N_filter=64, N_blocks=N_blocks) # 16 --> 8\n",
        "x = resblock_sequence_down(x, N_filter=128, N_blocks=N_blocks) # 8 --> 4\n",
        "\n",
        "x_spec = keras.layers.Flatten()(x)\n",
        "\n",
        "# Merge with latent\n",
        "x = keras.layers.Concatenate()([x_spec, latent_in])\n",
        "\n",
        "# Geometry generator path\n",
        "x = keras.layers.Dense(1*128)(x)\n",
        "x = keras.layers.Reshape((1, 128))(x)\n",
        "\n",
        "x = resblock_sequence_id(x, N_filter=128, N_blocks=N_blocks)\n",
        "\n",
        "x_m1 = keras.layers.Dense(512, activation='relu')(x)\n",
        "x_m1 = keras.layers.Dense(512, activation='relu')(x_m1)\n",
        "out_design_m1 = keras.layers.Dense(num_mats, activation='softmax', name='design_out_m1')(x_m1)\n",
        "\n",
        "x_m2 = keras.layers.Dense(512, activation='relu')(x)\n",
        "x_m2 = keras.layers.Dense(512, activation='relu')(x_m2)\n",
        "out_design_m2 = keras.layers.Dense(num_mats, activation='softmax', name='design_out_m2')(x_m2)\n",
        "\n",
        "x_t1 = keras.layers.Dense(512, activation='relu')(x)\n",
        "x_t1 = keras.layers.Dense(512, activation='relu')(x_t1)\n",
        "out_design_t1 = keras.layers.Dense(1, activation='linear', name='design_out_t1')(x_t1)\n",
        "\n",
        "x_t2 = keras.layers.Dense(512, activation='relu')(x)\n",
        "x_t2 = keras.layers.Dense(512, activation='relu')(x_t2)\n",
        "out_design_t2 = keras.layers.Dense(1, activation='linear', name='design_out_t2')(x_t2)\n",
        "\n",
        "decoder = keras.Model(\n",
        "    [latent_in, condition_target_spec_input],\n",
        "    [out_design_m1, out_design_m2, out_design_t1, out_design_t2], name=\"decoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmAojXKek-8M"
      },
      "source": [
        "# Define the full cVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXfIqn6qkkhF",
        "outputId": "69f0e34f-5911-4a0c-8b91-c8b4fc8e6c9d"
      },
      "outputs": [],
      "source": [
        "# Using the encoder to generate a randomized latent vector\n",
        "z_mean_out, z_var_log_out, z_rnd_out = encoder([design_in_m1, design_in_m2, design_in_t1, design_in_t2, condition_target_spec_input])\n",
        "\n",
        "# Using the decoder to generate the geometry parameters\n",
        "design_pred_m1, design_pred_m2, design_pred_t1, design_pred_t2 = decoder([z_rnd_out, condition_target_spec_input])\n",
        "\n",
        "cVAE = keras.Model(\n",
        "    [design_in_m1, design_in_m2, design_in_t1, design_in_t2, condition_target_spec_input],\n",
        "    [design_pred_m1, design_pred_m2, design_pred_t1, design_pred_t2],\n",
        "    name=\"cVAE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKxRFOJClB-0"
      },
      "source": [
        "# cVAE losses: reconstruction and KL\n",
        "\n",
        "In order to regularize the latent space, we need to add a Kullback-Leibler (KL) divergence loss in addition to the MSE reconstruction loss. Adding custom loss functions in keras can be done via model.add_loss(). We also add loss metrics to show status messages during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-O4Xuc8kklY"
      },
      "outputs": [],
      "source": [
        "input_dim = np.product(x_train.shape[1:])\n",
        "latent_dim = np.product(z.shape[1:])\n",
        "beta = 0.001 # Weight of KL loss. If too high, reconstruction will suffer.\n",
        "\n",
        "# Material reconstruction loss\n",
        "def categorical_ce(y_true, y_pred):\n",
        "    cce = keras.losses.CategoricalCrossentropy(name='reconstr_loss_m1')\n",
        "    return cce(y_true, y_pred)\n",
        "\n",
        "reconstruction_loss_m1 = categorical_ce(design_in_m1, design_pred_m1)\n",
        "\n",
        "cVAE.add_loss(reconstruction_loss_m1)\n",
        "cVAE.add_metric(reconstruction_loss_m1, name='reconstr_loss_m1')\n",
        "\n",
        "m1_acc = keras.metrics.categorical_accuracy(design_in_m1, design_pred_m1)\n",
        "cVAE.add_metric(m1_acc, name=\"m1_accuracy\")\n",
        "\n",
        "def categorical_ce1(y_true, y_pred):\n",
        "    cce1 = keras.losses.CategoricalCrossentropy(name='reconstr_loss_m2')\n",
        "    return cce1(y_true, y_pred)\n",
        "\n",
        "reconstruction_loss_m2 = categorical_ce1(design_in_m2, design_pred_m2)\n",
        "\n",
        "cVAE.add_loss(reconstruction_loss_m2)\n",
        "cVAE.add_metric(reconstruction_loss_m2, name='reconstr_loss_m2')\n",
        "\n",
        "m2_acc = keras.metrics.categorical_accuracy(design_in_m2, design_pred_m2)\n",
        "cVAE.add_metric(m2_acc, name=\"m2_accuracy\")\n",
        "\n",
        "\n",
        "# Thickness reconstruction loss\n",
        "reconstruction_loss_t1 = keras.losses.mse(\n",
        "    keras.backend.flatten(design_in_t1),\n",
        "    keras.backend.flatten(design_pred_t1))\n",
        "cVAE.add_loss(reconstruction_loss_t1)\n",
        "cVAE.add_metric(reconstruction_loss_t1, name='reconstr_loss_t1', aggregation='mean')\n",
        "\n",
        "reconstruction_loss_t2 = keras.losses.mse(\n",
        "    keras.backend.flatten(design_in_t2),\n",
        "    keras.backend.flatten(design_pred_t2))\n",
        "cVAE.add_loss(reconstruction_loss_t2)\n",
        "cVAE.add_metric(reconstruction_loss_t2, name='reconstr_loss_t2', aggregation='mean')\n",
        "\n",
        "# KL loss\n",
        "kl_loss = keras.backend.sum(\n",
        "    1 + z_var_log_out - keras.backend.square(z_mean_out) - keras.backend.exp(z_var_log_out), axis=-1)\n",
        "kl_loss = beta * keras.backend.mean(-0.5 / latent_dim * kl_loss)\n",
        "cVAE.add_loss(kl_loss)\n",
        "cVAE.add_metric(kl_loss, name='kl_loss', aggregation='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WAUQwSGlqWo"
      },
      "source": [
        "# Train the cVAE end-to-end\n",
        "\n",
        "In contrast to the 2-stage training of the Tandem, the cVAE is trained end-to-end in a single run. Note that we have now two inputs for the full cVAE model: designs and spectra, the model output are the reconstructed designs, which are compared to the input designs via MSE loss. After training, for inverse design we will use only the decoder part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBU6SXv2lr8W",
        "outputId": "38a49557-3a82-4736-dbff-fc4fcf931461"
      },
      "outputs": [],
      "source": [
        "# Compile the model using the Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "cVAE.compile(optimizer=optimizer)\n",
        "\n",
        "# Automatic learning rate reduction on loss plateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=4, verbose=1)\n",
        "# Callback for early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
        "callbacks = [reduce_lr, early_stop]\n",
        "\n",
        "# Fit the model with BN --> increasing batchsize schdedule\n",
        "hist = None # Global history after BS increase\n",
        "for i in range(3): # 3x16 epochs, doubling batchsize\n",
        "    _h = cVAE.fit(x=[x_train_m1, x_train_m2, x_train_t1, x_train_t2, y_train],\n",
        "                  y=[x_train_m1, x_train_m2, x_train_t1, x_train_t2],\n",
        "                  validation_split=0.2,\n",
        "                  batch_size=32 * 2**i, epochs=16,\n",
        "                  callbacks = callbacks)\n",
        "    if hist is None:\n",
        "        hist = _h\n",
        "    else:\n",
        "        # Update history\n",
        "        for k in hist.history:\n",
        "            hist.history[k] = np.concatenate([hist.history[k], _h.history[k]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4i5uWhCGpes"
      },
      "source": [
        "# Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "PGWM_kL0vLTw",
        "outputId": "843960b5-a6b7-415f-abe2-b953ca80bd7c"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(hist.history['loss'], label='loss')\n",
        "plt.plot(hist.history['val_loss'], label='val_loss')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlyQHx33DVBi"
      },
      "outputs": [],
      "source": [
        "cVAE.save('../Models/model_cVAE.h5')\n",
        "encoder.save('../Models/model_cVAE_encoder.h5')\n",
        "decoder.save('../Models/model_cVAE_decoder.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
